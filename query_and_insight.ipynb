{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights proposal\n",
    " \n",
    "## Insight 1:\n",
    "\n",
    "### Target:\n",
    "\n",
    "We are exploring the relationship between player position and their physical conditions\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "- We have chosen heights without shoes, weights, and body fat percentage as features to represent the physical trials of the player \n",
    "\n",
    "- We calculate the mean values on these attributes for each position group \n",
    "- We use bar chart to show the values in each group for better comparison\n",
    "\n",
    "### Result: (on average)\n",
    "1. for height: PG < SG < SF < PF < C\n",
    "2. for weight: PG < SG < SF < PF < C\n",
    "3. for body fat percentage: PG < SG < SF < PF < C\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "PG stands for point guards, whose primary role is to carry the ball across the perimeter, hence players in this position are on average shorter and lighter\n",
    "\n",
    "On the contrary, C (centers) are mainly responsible for blocking and defending, hence players in center position on average would be taller and heavier \n",
    "\n",
    "\n",
    "## Insight 2:\n",
    "\n",
    "### Target:\n",
    "\n",
    "We are exploring the relationship between player position and their athletic performace\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "- We have chosen max_vertical_leap, lane_agility_time, bench_press as features to represent the athletic performance of the player, as these features can be considered as proxy values for explosive power, speed and endurance\n",
    "\n",
    "- We calculate the mean values on these attributes for each position group \n",
    "- We use spider charts to see the performance in each dimension for each position \n",
    "\n",
    "### Result: (on average)\n",
    "1. C and PF: perform the best in bench press\n",
    "2. PG and SG: perform the best in lane agibility \n",
    "3. SG and PG: perform the best in vertical leap\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "Shooting Guards and Point Guards are responsible for shooting and dunking, hence they normally have the best jumping skills and explosive power \n",
    "\n",
    "Point Guards and  Shooting Guards require high agility to move pass the perimeter, hence they have the best speed and are the quickest players on the court  \n",
    "\n",
    "\n",
    "Center and Power Forwards require players are the most physically demanding positions, hence we can see players in these two positsion in general have greater endurance \n",
    "\n",
    "## Insight 3:\n",
    "\n",
    "### Target:\n",
    "\n",
    "We aim to analyse the distribution of NBA draft picks across different sources (college, high school, and others) to understand the primary recruitment channels for NBA teams.\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "- Data Categorisation: We categorise the source of each NBA draft pick into three groups: College, High School, and Other.\n",
    "- Visualisation: A bar chart is created to display the count of draft picks from each source, providing a clear view of the dominant recruitment channels.\n",
    "\n",
    "### Result:\n",
    "1. College recruits make up the majority of NBA draft picks.\n",
    "2. High School recruits have a relatively small representation compared to college recruits.\n",
    "3. Other sources provide some NBA talent but are less common than college pathways.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "The preference for players with collegiate experience reflects the NBAâ€™s reliance on college programs as a primary development ground for talent. This preference suggests that collegiate leagues provide a structured training environment that helps players build essential skills for the NBA.\n",
    "\n",
    "\n",
    "## Insight 4:\n",
    "\n",
    "### Target:\n",
    "\n",
    "We explore the stability of NBA franchises by assessing the duration that each city has hosted an NBA team, indicating strong or transient NBA markets.\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "- Data Cleaning: The dataset is cleaned to merge similar city names (e.g., combining \"Minneapolis\" and \"Minnesota\") and handle missing values by filling current years for active franchises.\n",
    "- Duration Calculation: We calculate the number of years each city has hosted a franchise and aggregate these durations for comparison.\n",
    "- Visualisation: A horizontal bar chart is used to display the total years of NBA presence for each city, sorted from longest to shortest duration.\n",
    "\n",
    "### Result:\n",
    "\n",
    "1. Los Angeles, New York, and Boston are among the cities with the longest NBA presence, indicating high franchise stability.\n",
    "2. Cities with shorter hosting durations may represent newer franchises or locations with more team movement.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "Cities with long-standing NBA franchises, such as Los Angeles and New York, reflect established markets that benefit from consistent fan engagement and revenue. The stability in these cities points to strong local support and significant market value, while newer or shorter-duration cities may still be developing their fan base and market presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## TODO: Use pathlib to get the path to the data directory. Path(__file__) won't work in a Jupyter notebook.\n",
    "DATA_PATH = Path.cwd() / 'data' / 'nba.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum columns displayed to the number of columns in your DataFrame\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sql.connect(DATA_PATH)  # connect to the database\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM draft_combine_stats\n",
    "\"\"\" \n",
    "\n",
    "df = pd.read_sql(query, con)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Proxy features for player's physical condition:\n",
    "- height_wo_shoes\n",
    "- weight \n",
    "- body_fat_pct\n",
    "\n",
    "Proxy features for player's athletic performance: \n",
    "- max_vertical_leap\n",
    "- lane_agility_time\n",
    "- bench_press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['position', 'height_wo_shoes', 'body_fat_pct', 'weight', 'max_vertical_leap', 'lane_agility_time', 'bench_press']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick data reporting\n",
    "\n",
    "1. missing values\n",
    "2. data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report on missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#report on data type\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling for draft_combine_stats (with selected columns)\n",
    "\n",
    "1. drop rows if target variable 'position' is empty \n",
    "2. convert object to float when necessary (for 'weight' column in specific)\n",
    "3. handle missing values in attribute columns (fill with mean)\n",
    "4. clean up / standardised labels (for 'position' column in specific)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_str_row(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    drop rows in the dataframe if a specified column is an empty string ''.\n",
    "\n",
    "    Arg:\n",
    "    df (pd.DataFrame): input dataframe\n",
    "    col (str): input column name \n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: dataframe after the clean-up\n",
    "    \"\"\"\n",
    "    return df[df[col] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = drop_empty_str_row(df, \"position\")\n",
    "print('number of filtered rows:', df.shape[0] - df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_float(df: pd.DataFrame, col_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    replace empty strings or None in the specified column with NaN \n",
    "    and converts the column to float\n",
    "\n",
    "    Arg:\n",
    "    df (pd.DataFrame): input dataframe\n",
    "    col_list (list): a list of column names to convert to float \n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: dataframe after the clean-up\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    for col in col_list:\n",
    "        df[col] = df[col].replace(['', None], np.nan)\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = convert_float(df_clean, [\"weight\", \"body_fat_pct\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_with_mean(df: pd.DataFrame, col_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    fill NaN values in the specified list of columns with the mean of each column.\n",
    "\n",
    "    Arg:\n",
    "    df (pd.DataFrame): input dataframe\n",
    "    col_list (list): a list of column names to fill NaN values in\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: dataframe after the processing \n",
    "    \"\"\"\n",
    "    for col in col_list:\n",
    "        mean_val = df[col].mean()\n",
    "        df[col] = df[col].fillna(mean_val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = fill_na_with_mean(df_clean, ['height_wo_shoes', \n",
    "'body_fat_pct', 'weight', 'max_vertical_leap', 'lane_agility_time', 'bench_press'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_position(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    clean up hybrid position labels \n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): input dataframe\n",
    "   \n",
    "    Returns:\n",
    "    pd.DataFrame: dataframe after the clean-up\n",
    "    \"\"\"\n",
    "    position_dict = {\n",
    "        'C-PF': 'C-PF',\n",
    "        'PF-C': 'C-PF',\n",
    "        'PF-SF': 'PF-SF',\n",
    "        'SF-PF': 'PF-SF',\n",
    "        'SG-PG': 'PG-SG',\n",
    "        'PG-SG': 'PG-SG',\n",
    "        'SF-SG': 'SG-SF',\n",
    "        'SG-SF': 'SG-SF'\n",
    "    }\n",
    "    df['position'] = df['position'].replace(position_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = cleanup_position(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['position'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_mean(df: pd.DataFrame, groupby_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    aggregate the df by a specified column and calculate the mean for other columns\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): input dataframe\n",
    "    groupby_col (str): column to perform groupby action on \n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: dataframe after the groupby action\n",
    "    \"\"\"\n",
    "    agg_df = df.groupby(groupby_col, as_index=False).mean()\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = groupby_mean(df_clean, \"position\")\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "1. bar chart to see average for each position \n",
    "2. radar chart for each position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight 1: position VS body conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_position(df: pd.DataFrame, metrics: list) -> None:\n",
    "    \"\"\"\n",
    "    plot bar chart for each specified metric by position\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): dataframe (after aggregation)\n",
    "    metrics (list): list of metrics (average value)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for metric in metrics:\n",
    "        #sort by values \n",
    "        sorted_df = df.sort_values(by=metric, ascending=False)\n",
    "        # plot bar chart\n",
    "        plt.figure(figsize=(6, 2))\n",
    "        plt.bar(sorted_df[\"position\"], sorted_df[metric])\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.title(f\"{metric} by Position\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_position(df_agg, ['height_wo_shoes', 'body_fat_pct', 'weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight 2: position VS athletic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rador_plot_position(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    create radar plots for each position\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): dataframe containing 'max_vertical_leap', 'lane_agility_time', 'bench_press'\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    #filter our hybrid position \n",
    "    df = df[df['position'].isin(['SG', 'PG', 'SF', 'PF', 'C'])]\n",
    "\n",
    "    # normalised and transform data\n",
    "    # max_vertical_leap: high means better explosive_power\n",
    "    # lane_agility_time: shorter means faster speed -> take the inverse\n",
    "    # bench_press: larger means greater endurance \n",
    "    min_leap = df['max_vertical_leap'].min()\n",
    "    max_leap = df['max_vertical_leap'].max()\n",
    "    min_agility = df['lane_agility_time'].min()\n",
    "    max_agility = df['lane_agility_time'].max()\n",
    "    min_press = df['bench_press'].min()\n",
    "    max_press = df['bench_press'].max()\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized['explosive_power'] = df['max_vertical_leap'].apply(lambda x: (x - min_leap) / (max_leap- min_leap))\n",
    "    df_normalized['speed'] = df['lane_agility_time'].apply(lambda x: (1/x - 1/max_agility) / (1/min_agility - 1/max_agility))\n",
    "    df_normalized['endurance'] = df['bench_press'].apply(lambda x: (x - min_press) / (max_press - min_press))\n",
    "\n",
    "    # Create radar plots for each position\n",
    "    for _, row in df_normalized.iterrows():\n",
    "        # Values for the radar chart, add the first value to close the loop\n",
    "        values = row[['explosive_power', 'speed', 'endurance']].values.tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "\n",
    "        # Define the angle for each axis\n",
    "        angles = np.linspace(0, 2 * np.pi, 3, endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "\n",
    "        # Initialize the radar chart\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "        # Draw the outline and fill in the area\n",
    "        ax.fill(angles, values, color='skyblue', alpha=0.25)\n",
    "        ax.plot(angles, values, color='blue', linewidth=2)\n",
    "\n",
    "        # Set labels for each metric\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(['explosive_power', 'speed', 'endurance'], fontsize=10)\n",
    "\n",
    "        # Set the title with the position name\n",
    "        position_name = row['position']\n",
    "        ax.set_title(f\"{position_name} Profile\", size=15, color='blue', y=1.1)\n",
    "\n",
    "        # Show each radar plot individually\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rador_plot_position(df_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight 3: NBA Draft Pick Distribution by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_draft_picks():\n",
    "    \"\"\"\n",
    "    This function loads draft pick data from a CSV file, categorizes draft picks by source\n",
    "    (College, High School, Others), and visualises the distribution in a bar chart.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM draft_history\n",
    "    \"\"\" \n",
    "\n",
    "    # Load the dataset\n",
    "    draft_data = pd.read_sql(query, con)\n",
    "\n",
    "\n",
    "    # Count the number of draft picks by organization type\n",
    "    draft_counts = draft_data['organization_type'].value_counts()\n",
    "\n",
    "    # Create categories for College, High School, and Others\n",
    "    categories = {\n",
    "        'College': draft_counts.get('College/University', 0),\n",
    "        'High School': draft_counts.get('High School', 0),\n",
    "        'Others': draft_counts.sum() - draft_counts.get('College/University', 0) - draft_counts.get('High School', 0)\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame and sort by the number of College draft picks\n",
    "    categories_df = pd.DataFrame(list(categories.items()), columns=['Category', 'Number of Draft Picks'])\n",
    "    categories_df = categories_df.sort_values(by='Number of Draft Picks', ascending=False)\n",
    "\n",
    "    # Visualization: Bar chart showing the number of draft picks by category\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(categories_df['Category'], categories_df['Number of Draft Picks'], color=['blue', 'green', 'red'])\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Number of Draft Picks\")\n",
    "    plt.title(\"Number of Draft Picks by Category\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "analyse_draft_picks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight 4: Stability of NBA Franchises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_franchise_by_city():\n",
    "    \"\"\"\n",
    "    This function loads draft pick data from a CSV file, categorizes draft picks by source\n",
    "    (College, High School, Others), and visualises the distribution in a bar chart.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM team_history\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Load the dataset\n",
    "    nba_data = pd.read_sql(query,con)\n",
    "\n",
    "    # Display the first few rows to understand the structure (for testing)\n",
    "    # print(nba_data.head())\n",
    "\n",
    "    # Data Cleaning Adjustments\n",
    "\n",
    "    # 1. Combine \"Minneapolis\" and \"Minnesota\" as \"Minnesota\"\n",
    "    nba_data['city'] = nba_data['city'].replace({'Minneapolis': 'Minnesota', 'Minnesota': 'Minnesota'})\n",
    "\n",
    "    # 2. Remove any blank or missing entries in the 'city' column\n",
    "    nba_data = nba_data[nba_data['city'].notna() & (nba_data['city'] != '')]\n",
    "\n",
    "    # 3. Merge \"Kansas City-Omaha\" and \"Kansas City\" into \"Kansas City\"\n",
    "    nba_data['city'] = nba_data['city'].replace({'Kansas City-Omaha': 'Kansas City', 'Kansas City': 'Kansas City'})\n",
    "\n",
    "    # 4. Remove \"Capital\" if it appears in the city names\n",
    "    nba_data['city'] = nba_data['city'].str.replace('Capital', '', regex=False).str.strip()\n",
    "\n",
    "    # Convert 'year_founded' and 'year_active_till' to datetime\n",
    "    nba_data['year_founded'] = pd.to_datetime(nba_data['year_founded'], format='%Y')\n",
    "    nba_data['year_active_till'] = pd.to_datetime(nba_data['year_active_till'], format='%Y', errors='coerce')\n",
    "\n",
    "    # Fill any missing 'year_active_till' values with the current year (assuming these teams are still active)\n",
    "    current_year = datetime.now().year\n",
    "    nba_data['year_active_till'] = nba_data['year_active_till'].fillna(pd.Timestamp(current_year))\n",
    "\n",
    "    # Calculate the number of years each team has been in its city\n",
    "    nba_data['years_in_city'] = (nba_data['year_active_till'].dt.year - nba_data['year_founded'].dt.year).astype(int)\n",
    "\n",
    "    # Aggregate years by city (regardless of franchise name) and sort by total years in descending order\n",
    "    city_duration = nba_data.groupby('city')['years_in_city'].sum().reset_index()\n",
    "    city_duration = city_duration.sort_values(by='years_in_city', ascending=False)\n",
    "\n",
    "    # Visualization: Total years each city has hosted a franchise (sorted)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(city_duration['city'], city_duration['years_in_city'], color='skyblue')\n",
    "    plt.xlabel('Total Years as Host City')\n",
    "    plt.ylabel('City')\n",
    "    plt.title('Total Years Each City Has Hosted an NBA Franchise (Sorted by Duration)')\n",
    "    plt.gca().invert_yaxis()  # To have the longest-serving cities at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "analyse_franchise_by_city()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
